{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ignorar:\n",
    "\n",
    "(mestrado guto):\n",
    "**Propostas da Prof. Karin:**\n",
    "- Treinar um modelo de classificação e tentar atingir o Tone, parecido com o que tem no artigo \n",
    "    - Gap Classificação: \n",
    "        1. Desempenho Ruim do Classificador (treinar \"melhor\")\n",
    "        2. Desempenho ruim do fine-tuning por LLM\n",
    "- Implementar um método (fine tuning / eng. de prompt) que faça a tarefa de reescrita (empática/polite)\n",
    "    - Pode ter um gap aqui\n",
    "- Treinar um modelo pra identificar motivos das reviews (Agrupamentos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pandas\n",
    "# %pip install matplotlib\n",
    "# %pip install --upgrade numpy\n",
    "# %pip install seaborn\n",
    "# %pip install scikit-learn\n",
    "# %pip install tensorflow\n",
    "# %pip install ipywidgets\n",
    "# %pip install --upgrade gensim\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Inicio | Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from string import punctuation\n",
    "from keras.layers import TextVectorization\n",
    "from gensim import utils\n",
    "import gensim.models\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re # Regular Expression\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Stats:               Tone\n",
      "count  2500.000000\n",
      "mean      2.867200\n",
      "std       0.948854\n",
      "min       1.000000\n",
      "25%       2.000000\n",
      "50%       3.000000\n",
      "75%       3.000000\n",
      "max       5.000000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Venue</th>\n",
       "      <th>Review ID</th>\n",
       "      <th>review</th>\n",
       "      <th>Tone</th>\n",
       "      <th>Review URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ShitMyReviewerSay</td>\n",
       "      <td>NaN</td>\n",
       "      <td>It is early in the year, but difficult to imag...</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ShitMyReviewerSay</td>\n",
       "      <td>NaN</td>\n",
       "      <td>You do not use the empirical data for the anal...</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ShitMyReviewerSay</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I understand that Wikipedia is not the best so...</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ShitMyReviewerSay</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reviewer #1: 'The project can hardly be descri...</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ShitMyReviewerSay</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The figures are dishonest and not all that use...</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ShitMyReviewerSay</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Find your inner nerdâ€”it must be a big part o...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ShitMyReviewerSay</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[entire review] 'Research method is very impor...</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ShitMyReviewerSay</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Some papers are a pleasure to read. This is no...</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ShitMyReviewerSay</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sorry guys, I'm throwing in the towel.</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ShitMyReviewerSay</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nobody in their right mind would ever suggest ...</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Venue Review ID  \\\n",
       "0  ShitMyReviewerSay       NaN   \n",
       "1  ShitMyReviewerSay       NaN   \n",
       "2  ShitMyReviewerSay       NaN   \n",
       "3  ShitMyReviewerSay       NaN   \n",
       "4  ShitMyReviewerSay       NaN   \n",
       "5  ShitMyReviewerSay       NaN   \n",
       "6  ShitMyReviewerSay       NaN   \n",
       "7  ShitMyReviewerSay       NaN   \n",
       "8  ShitMyReviewerSay       NaN   \n",
       "9  ShitMyReviewerSay       NaN   \n",
       "\n",
       "                                              review  Tone Review URL  \n",
       "0  It is early in the year, but difficult to imag...     2        NaN  \n",
       "1  You do not use the empirical data for the anal...     2        NaN  \n",
       "2  I understand that Wikipedia is not the best so...     3        NaN  \n",
       "3  Reviewer #1: 'The project can hardly be descri...     3        NaN  \n",
       "4  The figures are dishonest and not all that use...     2        NaN  \n",
       "5  Find your inner nerdâ€”it must be a big part o...     1        NaN  \n",
       "6  [entire review] 'Research method is very impor...     4        NaN  \n",
       "7  Some papers are a pleasure to read. This is no...     2        NaN  \n",
       "8             Sorry guys, I'm throwing in the towel.     1        NaN  \n",
       "9  Nobody in their right mind would ever suggest ...     2        NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'PolitenessDataset-FULL.csv'\n",
    "\n",
    "PolitenessDF = pd.read_csv(path)\n",
    "print('Data Stats:', PolitenessDF.describe())\n",
    "PolitenessDF.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum length of characters is: 552.0\n"
     ]
    }
   ],
   "source": [
    "# codificação\n",
    "test = PolitenessDF.copy()\n",
    "\n",
    "# codificação\n",
    "test['CodeVenue'] = test['Venue'].astype('category').cat.codes\n",
    "\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(test[['review','Tone']], test['Tone'], test_size=0.2, random_state=42)\n",
    "\n",
    "max_length = X_train['review'].str.len().max()\n",
    "print(f\"The maximum length of characters is: {max_length}\")\n",
    "\n",
    "# Lengths of each review\n",
    "lengths = X_train['review'].dropna().astype(str).str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some visuals\n",
    "\n",
    "# Plotting the distribution of the sources\n",
    "PolitenessDF['Venue'].value_counts().plot(title='Sources Freq', kind='bar')\n",
    "plt.show()\n",
    "\n",
    "# Tone distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(PolitenessDF['Tone'], bins=5, fill=True, color='blue', edgecolor='black')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.xticks(ticks=[1, 2, 3, 4, 5])\n",
    "plt.title(\"Histogram of Politeness Tones\", fontsize=16)\n",
    "plt.xlabel(\"Tone\", fontsize=14)\n",
    "plt.ylabel(\"Count\", fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "# Plot tone per venue, stacked\n",
    "PolitenessDF.groupby(['Venue', 'Tone']).size().unstack().plot(kind='bar', stacked=True, title='Tone per Venue')\n",
    "plt.show()\n",
    "\n",
    "# Plot histogram of review lengths\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(lengths, bins=30, kde=True, color='blue', alpha=0.7)  # Seaborn for enhanced visuals\n",
    "plt.title(\"Histogram of Review Lengths\", fontsize=16)\n",
    "plt.xlabel(\"Review Length (characters)\", fontsize=14)\n",
    "plt.ylabel(\"Frequency\", fontsize=14)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special Characters: !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "print('Special Characters:', punctuation)\n",
    "\n",
    "# stop_words = stopwords.words('portuguese')\n",
    "def preprocess(x: str):\n",
    "    new_x = x.replace(r'\"',' ')\n",
    "    for c in punctuation:\n",
    "        new_x = new_x.replace(c,' ')\n",
    "    pattern = r\"\"\"\n",
    "    [^\\w\\s]|         # Remove punctuation\n",
    "    http\\S+|         # Remove links\n",
    "    @\\w+|            # Remove mentions\n",
    "    #\\S+|            # Remove hashtags\n",
    "    \\b\\w*\\d\\w*\\b|    # Remove words containing numbers\n",
    "    \\s+              # Normalize spaces\n",
    "    \"\"\"    \n",
    "    new_x = re.sub(pattern, ' ', new_x, flags=re.VERBOSE) #removendo pontuação do texto\n",
    "    return new_x.lower().strip()\n",
    "\n",
    "df_train = X_train.dropna().copy()\n",
    "df_test = X_test.dropna().copy()\n",
    "\n",
    "## Pré-processar datasets de treino e teste\n",
    "## Dados de treino\n",
    "df_train['review_original'] = df_train['review']\n",
    "df_train['review'] = df_train['review'].apply(preprocess)\n",
    "\n",
    "## Dados de teste\n",
    "df_test['review_original'] = df_test['review']\n",
    "df_test['review'] = df_test['review'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This inclusion criteria is not needed due to the fact to be in a master program they would be legal adults. Please remove and adjust anywhere that this shows up.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Samples (só para ver o processo mesmo)\n",
    "df_train[df_train['Tone'] == 5].sample(1)['review_original'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x18add605eb0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## classe para montar o dataset\n",
    "class PreProcess:\n",
    "    def __init__(self, docs):\n",
    "            self.lista_text = docs\n",
    "    def __iter__(self):\n",
    "        for line in self.lista_text:\n",
    "            # assume there's one document per line, tokens separated by whitespace:\n",
    "            yield utils.simple_preprocess(line) # este método tokeniza e faz algum preprocessamento\n",
    "            # https://tedboy.github.io/nlps/generated/generated/gensim.utils.simple_preprocess.html\n",
    "\n",
    "sentences = PreProcess(df_train['review'].values)\n",
    "# assim treina o modelo usando as configurações padrão e estas especificadas aqui\n",
    "ModelWord2Vec = gensim.models.Word2Vec(sentences=sentences, vector_size=100, window=5, min_count=1, epochs=20, sg=1)\n",
    "\n",
    "ModelWord2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulário novo: 4857\n",
      "Vocabulário tem 4857 tokens. Os primeiros 5 tokens são:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', 'the', 'is', 'of']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adapta:\n",
    "MAX_LENGHT = 552\n",
    "vectorizer = TextVectorization(\n",
    "    max_tokens=20000, #vocabulário maximo\n",
    "    output_sequence_length=MAX_LENGHT,\n",
    "    )\n",
    "\n",
    "vectorizer.adapt(df_train['review'].dropna().astype(str).to_list())\n",
    "voc = vectorizer.get_vocabulary()\n",
    "print('Vocabulário novo:',len(voc))\n",
    "clean_voc = [str(word) for word in voc] \n",
    "\n",
    "# agora vemos os tokens do dataset começando pelos mais frequentes:\n",
    "print(f'Vocabulário tem {len(voc)} tokens. Os primeiros 5 tokens são:')\n",
    "voc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4857"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': 0,\n",
       " '[UNK]': 1,\n",
       " 'the': 2,\n",
       " 'is': 3,\n",
       " 'of': 4,\n",
       " 'to': 5,\n",
       " 'a': 6,\n",
       " 'and': 7,\n",
       " 'in': 8,\n",
       " 'this': 9,\n",
       " 'i': 10,\n",
       " 'paper': 11,\n",
       " 'it': 12,\n",
       " 'that': 13,\n",
       " 'not': 14,\n",
       " 'for': 15,\n",
       " 'be': 16,\n",
       " 'are': 17,\n",
       " 'on': 18,\n",
       " 'authors': 19,\n",
       " 'with': 20,\n",
       " 'have': 21,\n",
       " 'as': 22,\n",
       " 'but': 23,\n",
       " 'would': 24,\n",
       " 'an': 25,\n",
       " 'there': 26,\n",
       " 'or': 27,\n",
       " 'work': 28,\n",
       " 'you': 29,\n",
       " 'more': 30,\n",
       " 'by': 31,\n",
       " 'some': 32,\n",
       " 'results': 33,\n",
       " 'very': 34,\n",
       " 'can': 35,\n",
       " 'from': 36,\n",
       " 'my': 37,\n",
       " 'what': 38,\n",
       " 'they': 39,\n",
       " 'manuscript': 40,\n",
       " 'well': 41,\n",
       " 'if': 42,\n",
       " 'which': 43,\n",
       " 'should': 44,\n",
       " 'no': 45,\n",
       " 'proposed': 46,\n",
       " 'also': 47,\n",
       " 'do': 48,\n",
       " 'all': 49,\n",
       " 'like': 50,\n",
       " 'interesting': 51,\n",
       " 'was': 52,\n",
       " 'data': 53,\n",
       " 'at': 54,\n",
       " 'model': 55,\n",
       " 'comments': 56,\n",
       " 'so': 57,\n",
       " 'does': 58,\n",
       " 'has': 59,\n",
       " 't': 60,\n",
       " 'see': 61,\n",
       " 'how': 62,\n",
       " 'one': 63,\n",
       " 'method': 64,\n",
       " 'clear': 65,\n",
       " 'seems': 66,\n",
       " 'your': 67,\n",
       " 'could': 68,\n",
       " 'written': 69,\n",
       " 'these': 70,\n",
       " 'their': 71,\n",
       " 'its': 72,\n",
       " 'than': 73,\n",
       " 'other': 74,\n",
       " 'however': 75,\n",
       " 'problem': 76,\n",
       " 'why': 77,\n",
       " 'section': 78,\n",
       " 's': 79,\n",
       " 'good': 80,\n",
       " 'been': 81,\n",
       " 'any': 82,\n",
       " 'think': 83,\n",
       " 'use': 84,\n",
       " 'study': 85,\n",
       " 'much': 86,\n",
       " 'make': 87,\n",
       " 'am': 88,\n",
       " 'about': 89,\n",
       " 'read': 90,\n",
       " 'experiments': 91,\n",
       " 'research': 92,\n",
       " 'methods': 93,\n",
       " 'only': 94,\n",
       " 'learning': 95,\n",
       " 'author': 96,\n",
       " 'new': 97,\n",
       " 'important': 98,\n",
       " 'even': 99,\n",
       " 'approach': 100,\n",
       " 'training': 101,\n",
       " 'performance': 102,\n",
       " 'while': 103,\n",
       " 'when': 104,\n",
       " 'networks': 105,\n",
       " 'task': 106,\n",
       " 'such': 107,\n",
       " 'show': 108,\n",
       " 'please': 109,\n",
       " 'me': 110,\n",
       " 'idea': 111,\n",
       " 'contribution': 112,\n",
       " 'reviewer': 113,\n",
       " 'will': 114,\n",
       " 'review': 115,\n",
       " 'may': 116,\n",
       " 'first': 117,\n",
       " 'different': 118,\n",
       " 'state': 119,\n",
       " 'better': 120,\n",
       " 'were': 121,\n",
       " 'many': 122,\n",
       " 'algorithm': 123,\n",
       " 'image': 124,\n",
       " 'we': 125,\n",
       " 'two': 126,\n",
       " 'reading': 127,\n",
       " 'overall': 128,\n",
       " 'here': 129,\n",
       " 'given': 130,\n",
       " 'network': 131,\n",
       " 'main': 132,\n",
       " 'used': 133,\n",
       " 'over': 134,\n",
       " 'based': 135,\n",
       " 'time': 136,\n",
       " 'tasks': 137,\n",
       " 'figure': 138,\n",
       " 'too': 139,\n",
       " 'publication': 140,\n",
       " 'then': 141,\n",
       " 'information': 142,\n",
       " 'find': 143,\n",
       " 'bit': 144,\n",
       " 'up': 145,\n",
       " 'really': 146,\n",
       " 'example': 147,\n",
       " 'did': 148,\n",
       " 'classification': 149,\n",
       " 'using': 150,\n",
       " 'out': 151,\n",
       " 'current': 152,\n",
       " 'theoretical': 153,\n",
       " 'rather': 154,\n",
       " 'most': 155,\n",
       " 'models': 156,\n",
       " 'layer': 157,\n",
       " 'just': 158,\n",
       " 'deep': 159,\n",
       " 'between': 160,\n",
       " 'quite': 161,\n",
       " 'presented': 162,\n",
       " 'loss': 163,\n",
       " 'hard': 164,\n",
       " 'experimental': 165,\n",
       " 'because': 166,\n",
       " 'analysis': 167,\n",
       " 'scientific': 168,\n",
       " 'same': 169,\n",
       " 'result': 170,\n",
       " 'need': 171,\n",
       " 'into': 172,\n",
       " 'writing': 173,\n",
       " 'without': 174,\n",
       " 'useful': 175,\n",
       " 'understand': 176,\n",
       " 'literature': 177,\n",
       " 'journal': 178,\n",
       " 'further': 179,\n",
       " 'addressed': 180,\n",
       " 'had': 181,\n",
       " 'et': 182,\n",
       " 'enough': 183,\n",
       " 'clearly': 184,\n",
       " 'case': 185,\n",
       " 'article': 186,\n",
       " 'al': 187,\n",
       " 'e': 188,\n",
       " 'being': 189,\n",
       " 'novel': 190,\n",
       " 'neural': 191,\n",
       " 'might': 192,\n",
       " 'less': 193,\n",
       " 'instead': 194,\n",
       " 'believe': 195,\n",
       " 'them': 196,\n",
       " 'sure': 197,\n",
       " 'suggest': 198,\n",
       " 'simple': 199,\n",
       " 'seem': 200,\n",
       " 'recommend': 201,\n",
       " 'problems': 202,\n",
       " 'introduction': 203,\n",
       " 'high': 204,\n",
       " 'few': 205,\n",
       " 'existing': 206,\n",
       " 'discussion': 207,\n",
       " 'relevant': 208,\n",
       " 'question': 209,\n",
       " 'point': 210,\n",
       " 'number': 211,\n",
       " 'major': 212,\n",
       " 'little': 213,\n",
       " 'improved': 214,\n",
       " 'where': 215,\n",
       " 'way': 216,\n",
       " 'representation': 217,\n",
       " 'questions': 218,\n",
       " 'previous': 219,\n",
       " 'noise': 220,\n",
       " 'mean': 221,\n",
       " 'made': 222,\n",
       " 'language': 223,\n",
       " 'form': 224,\n",
       " 'done': 225,\n",
       " 'topic': 226,\n",
       " 'though': 227,\n",
       " 'text': 228,\n",
       " 'references': 229,\n",
       " 'part': 230,\n",
       " 'novelty': 231,\n",
       " 'needs': 232,\n",
       " 'lot': 233,\n",
       " 'limited': 234,\n",
       " 'improve': 235,\n",
       " 'difficult': 236,\n",
       " 'compared': 237,\n",
       " 'art': 238,\n",
       " 'architecture': 239,\n",
       " 'add': 240,\n",
       " 'actually': 241,\n",
       " 'unfortunately': 242,\n",
       " 'studies': 243,\n",
       " 'small': 244,\n",
       " 'since': 245,\n",
       " 'presentation': 246,\n",
       " 'poorly': 247,\n",
       " 'original': 248,\n",
       " 'optimization': 249,\n",
       " 'nothing': 250,\n",
       " 'line': 251,\n",
       " 'least': 252,\n",
       " 'lack': 253,\n",
       " 'know': 254,\n",
       " 'functions': 255,\n",
       " 'dataset': 256,\n",
       " 'd': 257,\n",
       " 'consider': 258,\n",
       " 'class': 259,\n",
       " 'whether': 260,\n",
       " 'sense': 261,\n",
       " 'reason': 262,\n",
       " 'quality': 263,\n",
       " 'perhaps': 264,\n",
       " 'now': 265,\n",
       " 'never': 266,\n",
       " 'interest': 267,\n",
       " 'his': 268,\n",
       " 'gradient': 269,\n",
       " 'error': 270,\n",
       " 'entropy': 271,\n",
       " 'don': 272,\n",
       " 'comparison': 273,\n",
       " 'cannot': 274,\n",
       " 'best': 275,\n",
       " 'although': 276,\n",
       " 'abstract': 277,\n",
       " 'submission': 278,\n",
       " 'similar': 279,\n",
       " 'sentence': 280,\n",
       " 're': 281,\n",
       " 'provide': 282,\n",
       " 'page': 283,\n",
       " 'non': 284,\n",
       " 'level': 285,\n",
       " 'general': 286,\n",
       " 'g': 287,\n",
       " 'distribution': 288,\n",
       " 'baseline': 289,\n",
       " 'want': 290,\n",
       " 'value': 291,\n",
       " 'unclear': 292,\n",
       " 'title': 293,\n",
       " 'terms': 294,\n",
       " 'table': 295,\n",
       " 'summary': 296,\n",
       " 'still': 297,\n",
       " 'standard': 298,\n",
       " 'specific': 299,\n",
       " 'propose': 300,\n",
       " 'present': 301,\n",
       " 'our': 302,\n",
       " 'nice': 303,\n",
       " 'long': 304,\n",
       " 'large': 305,\n",
       " 'issues': 306,\n",
       " 'graph': 307,\n",
       " 'follow': 308,\n",
       " 'fact': 309,\n",
       " 'examples': 310,\n",
       " 'convincing': 311,\n",
       " 'after': 312,\n",
       " 'weights': 313,\n",
       " 'thank': 314,\n",
       " 'something': 315,\n",
       " 'single': 316,\n",
       " 'significant': 317,\n",
       " 'science': 318,\n",
       " 'samples': 319,\n",
       " 'real': 320,\n",
       " 'papers': 321,\n",
       " 'motivation': 322,\n",
       " 'missing': 323,\n",
       " 'ideas': 324,\n",
       " 'framework': 325,\n",
       " 'found': 326,\n",
       " 'feel': 327,\n",
       " 'exploration': 328,\n",
       " 'end': 329,\n",
       " 'easy': 330,\n",
       " 'each': 331,\n",
       " 'details': 332,\n",
       " 'correct': 333,\n",
       " 'conclusions': 334,\n",
       " 'concerns': 335,\n",
       " 'both': 336,\n",
       " 'accepted': 337,\n",
       " 'â': 338,\n",
       " 'x': 339,\n",
       " 'weak': 340,\n",
       " 'thus': 341,\n",
       " 'through': 342,\n",
       " 'therefore': 343,\n",
       " 'term': 344,\n",
       " 'style': 345,\n",
       " 'statistical': 346,\n",
       " 'simply': 347,\n",
       " 'several': 348,\n",
       " 'revised': 349,\n",
       " 'policy': 350,\n",
       " 'parameters': 351,\n",
       " 'opinion': 352,\n",
       " 'net': 353,\n",
       " 'minor': 354,\n",
       " 'knowledge': 355,\n",
       " 'field': 356,\n",
       " 'features': 357,\n",
       " 'especially': 358,\n",
       " 'empirical': 359,\n",
       " 'before': 360,\n",
       " 'attached': 361,\n",
       " 'already': 362,\n",
       " 'version': 363,\n",
       " 'understanding': 364,\n",
       " 'theory': 365,\n",
       " 'somewhat': 366,\n",
       " 'reviewers': 367,\n",
       " 'related': 368,\n",
       " 'reads': 369,\n",
       " 'published': 370,\n",
       " 'proposes': 371,\n",
       " 'perform': 372,\n",
       " 'paragraph': 373,\n",
       " 'nor': 374,\n",
       " 'negative': 375,\n",
       " 'makes': 376,\n",
       " 'linear': 377,\n",
       " 'last': 378,\n",
       " 'explain': 379,\n",
       " 'english': 380,\n",
       " 'effort': 381,\n",
       " 'due': 382,\n",
       " 'clarity': 383,\n",
       " 'claims': 384,\n",
       " 'changes': 385,\n",
       " 'algorithms': 386,\n",
       " 'able': 387,\n",
       " 'trying': 388,\n",
       " 'strong': 389,\n",
       " 'shows': 390,\n",
       " 'set': 391,\n",
       " 'second': 392,\n",
       " 'r': 393,\n",
       " 'presents': 394,\n",
       " 'piece': 395,\n",
       " 'p': 396,\n",
       " 'often': 397,\n",
       " 'multi': 398,\n",
       " 'layers': 399,\n",
       " 'input': 400,\n",
       " 'hidden': 401,\n",
       " 'future': 402,\n",
       " 'focus': 403,\n",
       " 'concern': 404,\n",
       " 'comment': 405,\n",
       " 'area': 406,\n",
       " 'approaches': 407,\n",
       " 'appendix': 408,\n",
       " 'anything': 409,\n",
       " 'adding': 410,\n",
       " 'wrong': 411,\n",
       " 'who': 412,\n",
       " 'under': 413,\n",
       " 'those': 414,\n",
       " 'step': 415,\n",
       " 'sound': 416,\n",
       " 'self': 417,\n",
       " 'say': 418,\n",
       " 'regularization': 419,\n",
       " 'reference': 420,\n",
       " 'reader': 421,\n",
       " 'provided': 422,\n",
       " 'potentially': 423,\n",
       " 'poor': 424,\n",
       " 'points': 425,\n",
       " 'name': 426,\n",
       " 'looks': 427,\n",
       " 'learn': 428,\n",
       " 'kind': 429,\n",
       " 'issue': 430,\n",
       " 'improvement': 431,\n",
       " 'he': 432,\n",
       " 'go': 433,\n",
       " 'experiment': 434,\n",
       " 'exactly': 435,\n",
       " 'equation': 436,\n",
       " 'dont': 437,\n",
       " 'consistent': 438,\n",
       " 'clevr': 439,\n",
       " 'claim': 440,\n",
       " 'appears': 441,\n",
       " 'addition': 442,\n",
       " 'acceptance': 443,\n",
       " 'above': 444,\n",
       " 'word': 445,\n",
       " 'uses': 446,\n",
       " 'trained': 447,\n",
       " 'train': 448,\n",
       " 'thought': 449,\n",
       " 'things': 450,\n",
       " 'test': 451,\n",
       " 'structured': 452,\n",
       " 'short': 453,\n",
       " 'sentences': 454,\n",
       " 'risk': 455,\n",
       " 'required': 456,\n",
       " 'put': 457,\n",
       " 'proposal': 458,\n",
       " 'properly': 459,\n",
       " 'proof': 460,\n",
       " 'possible': 461,\n",
       " 'own': 462,\n",
       " 'order': 463,\n",
       " 'nonlinear': 464,\n",
       " 'm': 465,\n",
       " 'iclr': 466,\n",
       " 'ica': 467,\n",
       " 'great': 468,\n",
       " 'get': 469,\n",
       " 'generalization': 470,\n",
       " 'function': 471,\n",
       " 'figures': 472,\n",
       " 'evaluation': 473,\n",
       " 'estimate': 474,\n",
       " 'either': 475,\n",
       " 'domain': 476,\n",
       " 'described': 477,\n",
       " 'contributions': 478,\n",
       " 'conclusion': 479,\n",
       " 'completely': 480,\n",
       " 'around': 481,\n",
       " 'argument': 482,\n",
       " 'architectures': 483,\n",
       " 'analyses': 484,\n",
       " 'against': 485,\n",
       " 'adversarial': 486,\n",
       " 'actual': 487,\n",
       " 'accuracy': 488,\n",
       " 'words': 489,\n",
       " 'whole': 490,\n",
       " 'weight': 491,\n",
       " 'w': 492,\n",
       " 'visual': 493,\n",
       " 'variance': 494,\n",
       " 'try': 495,\n",
       " 'times': 496,\n",
       " 'system': 497,\n",
       " 'suitable': 498,\n",
       " 'structure': 499,\n",
       " 'significantly': 500,\n",
       " 'shown': 501,\n",
       " 'reported': 502,\n",
       " 'reject': 503,\n",
       " 'regression': 504,\n",
       " 'random': 505,\n",
       " 'provides': 506,\n",
       " 'properties': 507,\n",
       " 'parts': 508,\n",
       " 'none': 509,\n",
       " 'neither': 510,\n",
       " 'needed': 511,\n",
       " 'mention': 512,\n",
       " 'matrix': 513,\n",
       " 'making': 514,\n",
       " 'limitations': 515,\n",
       " 'job': 516,\n",
       " 'instance': 517,\n",
       " 'generally': 518,\n",
       " 'gan': 519,\n",
       " 'final': 520,\n",
       " 'estimator': 521,\n",
       " 'errors': 522,\n",
       " 'difference': 523,\n",
       " 'detail': 524,\n",
       " 'description': 525,\n",
       " 'defined': 526,\n",
       " 'dear': 527,\n",
       " 'convex': 528,\n",
       " 'cons': 529,\n",
       " 'c': 530,\n",
       " 'beyond': 531,\n",
       " 'baselines': 532,\n",
       " 'bad': 533,\n",
       " 'back': 534,\n",
       " 'answer': 535,\n",
       " 'another': 536,\n",
       " 'amount': 537,\n",
       " 'again': 538,\n",
       " 'addressing': 539,\n",
       " 'added': 540,\n",
       " 'years': 541,\n",
       " 'thanks': 542,\n",
       " 'synthetic': 543,\n",
       " 'significance': 544,\n",
       " 'sample': 545,\n",
       " 'right': 546,\n",
       " 'revision': 547,\n",
       " 'relatively': 548,\n",
       " 'rejected': 549,\n",
       " 'prior': 550,\n",
       " 'obvious': 551,\n",
       " 'noisy': 552,\n",
       " 'multiple': 553,\n",
       " 'mind': 554,\n",
       " 'mi': 555,\n",
       " 'maybe': 556,\n",
       " 'lu': 557,\n",
       " 'low': 558,\n",
       " 'indeed': 559,\n",
       " 'including': 560,\n",
       " 'impact': 561,\n",
       " 'give': 562,\n",
       " 'generalize': 563,\n",
       " 'fully': 564,\n",
       " 'fig': 565,\n",
       " 'far': 566,\n",
       " 'evidence': 567,\n",
       " 'evaluate': 568,\n",
       " 'differences': 569,\n",
       " 'context': 570,\n",
       " 'b': 571,\n",
       " 'attempt': 572,\n",
       " 'assumptions': 573,\n",
       " 'write': 574,\n",
       " 'world': 575,\n",
       " 'via': 576,\n",
       " 'vague': 577,\n",
       " 'trust': 578,\n",
       " 'thing': 579,\n",
       " 'testing': 580,\n",
       " 'technique': 581,\n",
       " 'technical': 582,\n",
       " 'support': 583,\n",
       " 'student': 584,\n",
       " 'space': 585,\n",
       " 'solving': 586,\n",
       " 'sampling': 587,\n",
       " 'report': 588,\n",
       " 'relative': 589,\n",
       " 'relationships': 590,\n",
       " 'reconstruction': 591,\n",
       " 'program': 592,\n",
       " 'probably': 593,\n",
       " 'pretty': 594,\n",
       " 'practice': 595,\n",
       " 'performed': 596,\n",
       " 'per': 597,\n",
       " 'parameter': 598,\n",
       " 'outperforms': 599,\n",
       " 'observed': 600,\n",
       " 'note': 601,\n",
       " 'necessary': 602,\n",
       " 'n': 603,\n",
       " 'mostly': 604,\n",
       " 'mistakes': 605,\n",
       " 'misleading': 606,\n",
       " 'meant': 607,\n",
       " 'machine': 608,\n",
       " 'limitation': 609,\n",
       " 'lead': 610,\n",
       " 'known': 611,\n",
       " 'key': 612,\n",
       " 'included': 613,\n",
       " 'images': 614,\n",
       " 'im': 615,\n",
       " 'furthermore': 616,\n",
       " 'full': 617,\n",
       " 'formulation': 618,\n",
       " 'file': 619,\n",
       " 'feature': 620,\n",
       " 'fairly': 621,\n",
       " 'expect': 622,\n",
       " 'entire': 623,\n",
       " 'effect': 624,\n",
       " 'dropping': 625,\n",
       " 'doesnt': 626,\n",
       " 'discussed': 627,\n",
       " 'design': 628,\n",
       " 'convolutional': 629,\n",
       " 'considered': 630,\n",
       " 'confusing': 631,\n",
       " 'close': 632,\n",
       " 'clinical': 633,\n",
       " 'cite': 634,\n",
       " 'carefully': 635,\n",
       " 'bound': 636,\n",
       " 'big': 637,\n",
       " 'behind': 638,\n",
       " 'application': 639,\n",
       " 'additional': 640,\n",
       " 'across': 641,\n",
       " 'works': 642,\n",
       " 'wise': 643,\n",
       " 'variational': 644,\n",
       " 'true': 645,\n",
       " 'thorough': 646,\n",
       " 'thesis': 647,\n",
       " 'taken': 648,\n",
       " 'take': 649,\n",
       " 'suggestions': 650,\n",
       " 'substantial': 651,\n",
       " 'stronger': 652,\n",
       " 'states': 653,\n",
       " 'stated': 654,\n",
       " 'sort': 655,\n",
       " 'solve': 656,\n",
       " 'social': 657,\n",
       " 'saying': 658,\n",
       " 'rnn': 659,\n",
       " 'rl': 660,\n",
       " 'revisions': 661,\n",
       " 'relational': 662,\n",
       " 'relation': 663,\n",
       " 'readers': 664,\n",
       " 'perspective': 665,\n",
       " 'patients': 666,\n",
       " 'particularly': 667,\n",
       " 'natural': 668,\n",
       " 'mutual': 669,\n",
       " 'must': 670,\n",
       " 'mlp': 671,\n",
       " 'min': 672,\n",
       " 'measure': 673,\n",
       " 'meaningful': 674,\n",
       " 'mainly': 675,\n",
       " 'later': 676,\n",
       " 'justification': 677,\n",
       " 'independent': 678,\n",
       " 'higher': 679,\n",
       " 'helpful': 680,\n",
       " 'help': 681,\n",
       " 'having': 682,\n",
       " 'group': 683,\n",
       " 'grammatical': 684,\n",
       " 'going': 685,\n",
       " 'following': 686,\n",
       " 'findings': 687,\n",
       " 'finding': 688,\n",
       " 'finally': 689,\n",
       " 'essentially': 690,\n",
       " 'doing': 691,\n",
       " 'doesn': 692,\n",
       " 'despite': 693,\n",
       " 'descent': 694,\n",
       " 'datasets': 695,\n",
       " 'counting': 696,\n",
       " 'content': 697,\n",
       " 'consistency': 698,\n",
       " 'conference': 699,\n",
       " 'community': 700,\n",
       " 'comes': 701,\n",
       " 'cases': 702,\n",
       " 'audience': 703,\n",
       " 'attention': 704,\n",
       " 'assumption': 705,\n",
       " 'assume': 706,\n",
       " 'applied': 707,\n",
       " 'applications': 708,\n",
       " 'address': 709,\n",
       " 'activation': 710,\n",
       " 'zero': 711,\n",
       " 'yet': 712,\n",
       " 'y': 713,\n",
       " 'within': 714,\n",
       " 'view': 715,\n",
       " 'variables': 716,\n",
       " 'usually': 717,\n",
       " 'until': 718,\n",
       " 'typos': 719,\n",
       " 'type': 720,\n",
       " 'trpo': 721,\n",
       " 'totally': 722,\n",
       " 'top': 723,\n",
       " 'three': 724,\n",
       " 'theorem': 725,\n",
       " 'techniques': 726,\n",
       " 'surprising': 727,\n",
       " 'subspace': 728,\n",
       " 'straightforward': 729,\n",
       " 'statements': 730,\n",
       " 'statement': 731,\n",
       " 'sorry': 732,\n",
       " 'size': 733,\n",
       " 'simulation': 734,\n",
       " 'seen': 735,\n",
       " 'said': 736,\n",
       " 'reviews': 737,\n",
       " 'reviewed': 738,\n",
       " 'response': 739,\n",
       " 'representations': 740,\n",
       " 'regarding': 741,\n",
       " 'recurrent': 742,\n",
       " 'recent': 743,\n",
       " 'reasons': 744,\n",
       " 'project': 745,\n",
       " 'product': 746,\n",
       " 'positive': 747,\n",
       " 'past': 748,\n",
       " 'pages': 749,\n",
       " 'output': 750,\n",
       " 'off': 751,\n",
       " 'obtained': 752,\n",
       " 'objective': 753,\n",
       " 'norm': 754,\n",
       " 'nature': 755,\n",
       " 'methodology': 756,\n",
       " 'methodological': 757,\n",
       " 'mentioned': 758,\n",
       " 'memory': 759,\n",
       " 'max': 760,\n",
       " 'mature': 761,\n",
       " 'lstm': 762,\n",
       " 'lower': 763,\n",
       " 'lost': 764,\n",
       " 'look': 765,\n",
       " 'length': 766,\n",
       " 'learned': 767,\n",
       " 'larger': 768,\n",
       " 'lacking': 769,\n",
       " 'justified': 770,\n",
       " 'judge': 771,\n",
       " 'itself': 772,\n",
       " 'independence': 773,\n",
       " 'incremental': 774,\n",
       " 'increase': 775,\n",
       " 'include': 776,\n",
       " 'impressive': 777,\n",
       " 'impossible': 778,\n",
       " 'importance': 779,\n",
       " 'identity': 780,\n",
       " 'human': 781,\n",
       " 'hence': 782,\n",
       " 'h': 783,\n",
       " 'gcn': 784,\n",
       " 'gain': 785,\n",
       " 'fine': 786,\n",
       " 'exist': 787,\n",
       " 'energy': 788,\n",
       " 'embedding': 789,\n",
       " 'editor': 790,\n",
       " 'easily': 791,\n",
       " 'earlier': 792,\n",
       " 'down': 793,\n",
       " 'divergence': 794,\n",
       " 'determinant': 795,\n",
       " 'detailed': 796,\n",
       " 'demonstrate': 797,\n",
       " 'correctly': 798,\n",
       " 'convergence': 799,\n",
       " 'contains': 800,\n",
       " 'contain': 801,\n",
       " 'cnns': 802,\n",
       " 'cited': 803,\n",
       " 'change': 804,\n",
       " 'certain': 805,\n",
       " 'bottleneck': 806,\n",
       " 'benefit': 807,\n",
       " 'below': 808,\n",
       " 'avoid': 809,\n",
       " 'assessment': 810,\n",
       " 'aspects': 811,\n",
       " 'appropriate': 812,\n",
       " 'activations': 813,\n",
       " 'accept': 814,\n",
       " 'academic': 815,\n",
       " 'ablation': 816,\n",
       " 'z': 817,\n",
       " 'worth': 818,\n",
       " 'weaknesses': 819,\n",
       " 'vector': 820,\n",
       " 'variable': 821,\n",
       " 'vanishing': 822,\n",
       " 'v': 823,\n",
       " 'us': 824,\n",
       " 'unless': 825,\n",
       " 'themselves': 826,\n",
       " 'tested': 827,\n",
       " 'technically': 828,\n",
       " 'strengths': 829,\n",
       " 'stochastic': 830,\n",
       " 'started': 831,\n",
       " 'stack': 832,\n",
       " 'source': 833,\n",
       " 'sometimes': 834,\n",
       " 'sgd': 835,\n",
       " 'setting': 836,\n",
       " 'scope': 837,\n",
       " 'scale': 838,\n",
       " 'says': 839,\n",
       " 'satisfied': 840,\n",
       " 'rpn': 841,\n",
       " 'revise': 842,\n",
       " 'resulting': 843,\n",
       " 'recommendation': 844,\n",
       " 'reasonable': 845,\n",
       " 'ready': 846,\n",
       " 'rates': 847,\n",
       " 'quasi': 848,\n",
       " 'qualitative': 849,\n",
       " 'publish': 850,\n",
       " 'pros': 851,\n",
       " 'promising': 852,\n",
       " 'process': 853,\n",
       " 'previously': 854,\n",
       " 'practical': 855,\n",
       " 'places': 856,\n",
       " 'path': 857,\n",
       " 'particular': 858,\n",
       " 'outputs': 859,\n",
       " 'others': 860,\n",
       " 'organized': 861,\n",
       " 'operations': 862,\n",
       " 'omie': 863,\n",
       " 'old': 864,\n",
       " 'observations': 865,\n",
       " 'objectives': 866,\n",
       " 'notation': 867,\n",
       " 'necessarily': 868,\n",
       " 'motivated': 869,\n",
       " 'modeling': 870,\n",
       " 'mnist': 871,\n",
       " 'missed': 872,\n",
       " 'measured': 873,\n",
       " 'looking': 874,\n",
       " 'logic': 875,\n",
       " 'list': 876,\n",
       " 'liked': 877,\n",
       " 'left': 878,\n",
       " 'lacks': 879,\n",
       " 'l': 880,\n",
       " 'iterations': 881,\n",
       " 'isnt': 882,\n",
       " 'intuition': 883,\n",
       " 'insights': 884,\n",
       " 'inputs': 885,\n",
       " 'improvements': 886,\n",
       " 'imagine': 887,\n",
       " 'her': 888,\n",
       " 'hardly': 889,\n",
       " 'half': 890,\n",
       " 'gives': 891,\n",
       " 'gaussian': 892,\n",
       " 'game': 893,\n",
       " 'frankly': 894,\n",
       " 'forward': 895,\n",
       " 'followed': 896,\n",
       " 'focuses': 897,\n",
       " 'fast': 898,\n",
       " 'fails': 899,\n",
       " 'extremely': 900,\n",
       " 'explanation': 901,\n",
       " 'explained': 902,\n",
       " 'expert': 903,\n",
       " 'ever': 904,\n",
       " 'estimation': 905,\n",
       " 'estimating': 906,\n",
       " 'epsilon': 907,\n",
       " 'encoder': 908,\n",
       " 'early': 909,\n",
       " 'disappointment': 910,\n",
       " 'designed': 911,\n",
       " 'derivation': 912,\n",
       " 'definitely': 913,\n",
       " 'deal': 914,\n",
       " 'currently': 915,\n",
       " 'cover': 916,\n",
       " 'contribute': 917,\n",
       " 'compare': 918,\n",
       " 'combination': 919,\n",
       " 'clustering': 920,\n",
       " 'category': 921,\n",
       " 'called': 922,\n",
       " 'basic': 923,\n",
       " 'average': 924,\n",
       " 'alternative': 925,\n",
       " 'adequate': 926,\n",
       " 'adds': 927,\n",
       " 'achieves': 928,\n",
       " 'acceptable': 929,\n",
       " 'worst': 930,\n",
       " 'wonder': 931,\n",
       " 'whose': 932,\n",
       " 'whatever': 933,\n",
       " 'waste': 934,\n",
       " 'warrant': 935,\n",
       " 'values': 936,\n",
       " 'vae': 937,\n",
       " 'usual': 938,\n",
       " 'update': 939,\n",
       " 'trivial': 940,\n",
       " 'treatment': 941,\n",
       " 'transfer': 942,\n",
       " 'throughout': 943,\n",
       " 'thinking': 944,\n",
       " 'terrible': 945,\n",
       " 'terminology': 946,\n",
       " 'tell': 947,\n",
       " 'sufficient': 948,\n",
       " 'successfully': 949,\n",
       " 'submitted': 950,\n",
       " 'subgraph': 951,\n",
       " 'studied': 952,\n",
       " 'stop': 953,\n",
       " 'statistics': 954,\n",
       " 'starting': 955,\n",
       " 'spatial': 956,\n",
       " 'soon': 957,\n",
       " 'someone': 958,\n",
       " 'solutions': 959,\n",
       " 'solid': 960,\n",
       " 'sn': 961,\n",
       " 'shot': 962,\n",
       " 'she': 963,\n",
       " 'shall': 964,\n",
       " 'sets': 965,\n",
       " 'serious': 966,\n",
       " 'sequences': 967,\n",
       " 'sections': 968,\n",
       " 'score': 969,\n",
       " 'school': 970,\n",
       " 'run': 971,\n",
       " 'robust': 972,\n",
       " 'return': 973,\n",
       " 'restricted': 974,\n",
       " 'respect': 975,\n",
       " 'res': 976,\n",
       " 'requires': 977,\n",
       " 'require': 978,\n",
       " 'reports': 979,\n",
       " 'relevance': 980,\n",
       " 'relations': 981,\n",
       " 'reinforcement': 982,\n",
       " 'regularizer': 983,\n",
       " 'refer': 984,\n",
       " 'reduce': 985,\n",
       " 'rebuttal': 986,\n",
       " 'rate': 987,\n",
       " 'rarely': 988,\n",
       " 'raised': 989,\n",
       " 'predictive': 990,\n",
       " 'potential': 991,\n",
       " 'post': 992,\n",
       " 'possibly': 993,\n",
       " 'population': 994,\n",
       " 'pnl': 995,\n",
       " 'physics': 996,\n",
       " 'phrase': 997,\n",
       " 'phi': 998,\n",
       " 'performances': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# um dicionario com todas as palavras do vocabulario e seus indices de 0 a len(voc)-1\n",
    "word_index = dict(zip(clean_voc, range(len(clean_voc))))\n",
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### HYPERPARAMETERS (from notebooks)\n",
    "POLITENESS_LEVELS = 5\n",
    "EPOCHS = 30\n",
    "MAXLEN = 768 # Since SciBERT returns 768 embeddings vector\n",
    "LSTM_UNITS = 256\n",
    "is_BiLSTM = True # Flag to automate other pre-processing for With or Without BiLSTM variants\n",
    "VOCAB_LEN = 1853\n",
    "EMBEDDING_DIMENSION = 768\n",
    "\n",
    "import pickle\n",
    "\n",
    "# LOAD EMBEDS DATASET\n",
    "def loadPickle(name):\n",
    "    start_path = '../PolitePEER/'\n",
    "    if is_BiLSTM:\n",
    "        LOAD_PATH = start_path+'Tokennized_Processed_X_train-BiLSTM.csv'\n",
    "        train_embeds = pd.read_csv(LOAD_PATH)\n",
    "        \n",
    "        LOAD_PATH = start_path+'Tokennized_Processed_X_test-BiLSTM.csv'\n",
    "        test_embeds = pd.read_csv(LOAD_PATH)\n",
    "        \n",
    "        LOAD_PATH = start_path+'Tokennized_Processed_X_val-BiLSTM.csv'\n",
    "        val_embeds = pd.read_csv(LOAD_PATH)\n",
    "        \n",
    "    else:\n",
    "        LOAD_PATH = start_path+name+'_train.pickle'\n",
    "        with open(LOAD_PATH, 'rb') as handle:\n",
    "            train_embeds = pickle.load(handle)\n",
    "            handle.close()\n",
    "\n",
    "        LOAD_PATH = start_path+name+'_test.pickle'\n",
    "        with open(LOAD_PATH, 'rb') as handle:\n",
    "            test_embeds = pickle.load(handle)\n",
    "            handle.close()\n",
    "\n",
    "        LOAD_PATH = start_path+name+'_val.pickle'\n",
    "        with open(LOAD_PATH, 'rb') as handle:\n",
    "            val_embeds = pickle.load(handle)\n",
    "            handle.close()\n",
    "\n",
    "    y_train = pd.read_csv(start_path+'y_train.csv')\n",
    "    y_val = pd.read_csv(start_path+'y_val.csv')\n",
    "    y_test = pd.read_csv(start_path+'y_test.csv')\n",
    "\n",
    "    print('\\n***** LOADED '+ name+' *****\\n')\n",
    "    print(f'TRAIN SHAPE : {train_embeds.shape}\\nTEST SHAPE : {test_embeds.shape}\\nVAL SHAPE : {val_embeds.shape}\\nY-TRAIN SHAPE : {y_train.shape}\\nY-TEST SHAPE : {y_test.shape}\\nY-VAL SHAPE : {y_val.shape}')\n",
    "\n",
    "    return train_embeds, test_embeds, val_embeds, y_train, y_test, y_val\n",
    "\n",
    "# /kaggle/input/iitpolitenesslevels/SCIBERT_train.pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** LOADED  *****\n",
      "\n",
      "TRAIN SHAPE : (4556, 768)\n",
      "TEST SHAPE : (855, 768)\n",
      "VAL SHAPE : (284, 768)\n",
      "Y-TRAIN SHAPE : (4556, 5)\n",
      "Y-TEST SHAPE : (855, 5)\n",
      "Y-VAL SHAPE : (284, 5)\n"
     ]
    }
   ],
   "source": [
    "name = ''\n",
    "train_embeds, test_embeds, val_embeds, y_train, y_test, y_val = loadPickle(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#e687f1;\">**TODO**</span>: continuar daqui\n",
    "\n",
    "GPT suggestions, embeddings:\n",
    "\n",
    "Summary of Options:\n",
    "- BERT (Hugging Face): Contextual, state-of-the-art embeddings but computationally heavier.\n",
    "- GloVe/FastText: Simpler, pre-trained embeddings that are non-contextual but lightweight.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModel\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "sentences = df_train['review'].tolist()\n",
    "\n",
    "inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    # Extract the CLS token embeddings\n",
    "    sentence_embeddings = outputs.last_hidden_state[:, 0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.29      0.19      0.23        32\n",
      "           2       0.34      0.36      0.35        87\n",
      "           3       0.61      0.68      0.64       194\n",
      "           4       0.45      0.38      0.41        73\n",
      "           5       0.36      0.29      0.32        14\n",
      "\n",
      "    accuracy                           0.50       400\n",
      "   macro avg       0.41      0.38      0.39       400\n",
      "weighted avg       0.49      0.50      0.49       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "labels = df_train['Tone'].to_list()\n",
    "\n",
    "# Example: Split into 80% training and 20% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(sentence_embeddings.numpy(), labels, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Initialize and train the classifier\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gutog\\AppData\\Local\\Temp\\ipykernel_12060\\1021412078.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.embeddings = torch.tensor(embeddings, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 1.3079\n",
      "Epoch [2/10], Loss: 1.1273\n",
      "Epoch [3/10], Loss: 1.0356\n",
      "Epoch [4/10], Loss: 0.9749\n",
      "Epoch [5/10], Loss: 0.9299\n",
      "Epoch [6/10], Loss: 0.8586\n",
      "Epoch [7/10], Loss: 0.8154\n",
      "Epoch [8/10], Loss: 0.7239\n",
      "Epoch [9/10], Loss: 0.6499\n",
      "Epoch [10/10], Loss: 0.5679\n",
      "Accuracy: 48.75%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode labels to integers\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(sentence_embeddings, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Custom Dataset class\n",
    "class PolitenessDataset(Dataset):\n",
    "    def __init__(self, embeddings, labels):\n",
    "        self.embeddings = torch.tensor(embeddings, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.embeddings[idx], self.labels[idx]\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataset = PolitenessDataset(X_train, y_train)\n",
    "test_dataset = PolitenessDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Define BiLSTM model\n",
    "class BiLSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, dropout):\n",
    "        super(BiLSTMClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=n_layers, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)  # Multiply by 2 for bidirectional\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # LSTM layer\n",
    "        lstm_out, _ = self.lstm(x.unsqueeze(1))  # Add sequence dimension\n",
    "        lstm_out = self.dropout(lstm_out[:, -1, :])  # Use the last hidden state\n",
    "\n",
    "        # Fully connected layer\n",
    "        logits = self.fc(lstm_out)\n",
    "        return logits\n",
    "\n",
    "# Model parameters\n",
    "input_dim = 768  # Dimension of embeddings\n",
    "hidden_dim = 128  # Number of hidden units\n",
    "output_dim = len(np.unique(labels))  # Number of politeness levels\n",
    "n_layers = 2  # Number of LSTM layers\n",
    "dropout = 0.3  # Dropout rate\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "model = BiLSTMClassifier(input_dim, hidden_dim, output_dim, n_layers, dropout)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Training loop\n",
    "def train_model(model, train_loader, criterion, optimizer, device, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for embeddings, labels in train_loader:\n",
    "            embeddings, labels = embeddings.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(embeddings)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, criterion, optimizer, device, num_epochs=10)\n",
    "\n",
    "# Evaluation loop\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for embeddings, labels in test_loader:\n",
    "            embeddings, labels = embeddings.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(embeddings)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f\"Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(model, test_loader, device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
